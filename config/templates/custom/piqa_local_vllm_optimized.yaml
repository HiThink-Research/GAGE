api_version: gage/v1alpha1
kind: PipelineConfig
metadata:
  name: piqa_local_vllm_optimized
  description: |
    本地 vLLM AsyncLLMEngine 评测 PIQA 子集，演示顶层 backends + backend_id 引用、
    三层并发对齐（L1 任务并发 = L2 Role 并发 = L3 backend 批量能力），以及 Step 继承与推断。

custom:
  steps:
    - step: inference
      # 仅有一个 dut_model 角色时，可省略 adapter_id，由 TaskPlan 自动推断
    - step: auto_eval

datasets:
  - dataset_id: piqa_val
    loader: jsonl
    params:
      path: ${DATA_ROOT:-/mnt/aime_data_ssd/user_workspace/zhuwenqiao/hub/cmp/ths}/piqa_100.jsonl

backends:
  - backend_id: vllm_engine
    type: vllm
    config:
      model_path: ${VLLM_MODEL_PATH:-/mnt/model/qwen2_5_05B}
      tensor_parallel_size: 1
      max_batch_size: 32            # L3: 物理批处理能力
      async_max_concurrency: 32
      max_model_len: 2048
      gpu_memory_utilization: 0.9
      sampling_params:
        temperature: 0.0

role_adapters:
  - adapter_id: dut_qwen25_vllm
    role_type: dut_model
    backend_id: vllm_engine        # 顶层 backend 引用
    concurrency: 32                # L2: 角色级并发，与 backend 对齐
    capabilities:
      - chat_completion

metrics:
  - metric_id: piqa_acc
    implementation: multi_choice_accuracy
    params:
      label_field: sample.choices.0.message.content.0.text
      option_map_field: sample.metadata.option_map
      prediction_field: model_output.answer

tasks:
  - task_id: piqa_eval
    dataset_id: piqa_val
    max_samples: ${MAX_SAMPLES:-500}
    # 不显式声明 steps，默认继承 custom.steps 并在运行时补全 inference.adapter_id
    concurrency: 32                # L1: Task 调度并发，与 L2/L3 对齐
    reporting:
      sinks:
        - type: console
        - type: file
          params:
            output_path: ${SAVE_DIR:-./runs}/${metadata.name}_${task.task_id}_events.jsonl
