api_version: gage/v1alpha1
kind: PipelineConfig
metadata:
  name: mmlu_pro_vllm_async
  description: using vllm_backend eval mmlu_pro

custom:
  steps:
    - step: inference
    - step: auto_eval

datasets:
  - dataset_id: live_code_bench_ds
    hub: huggingface
    hub_params:
      hub_id: livecodebench/code_generation_lite
    loader: hf_hub_live_code_bench
    params:
      scenario: codegeneration
      not_fast: true
      limit: 10
      #start_date: 1990-01-01
      #end_date: 2027-01-01
      release_version: release_v1
      local_dir: /home/tione/notebook/zhuwenqiao_ssd/GAGE_workspace/data/my_local_data         
      preprocess: live_code_bench_chat_preprocessor
      preprocess_kwargs:
        n_few_shot: 5

backends:
  - backend_id: mmlu_pro_vllm_backend
    type: vllm
    config:
      model_path: /mnt/model/qwen2_5_1.5B
      tokenizer_path: /mnt/model/qwen2_5_1.5B
      force_tokenize_prompt: true
      tensor_parallel_size: 1
      max_tokens: 512
      sampling_params:
        n: 5
        temperature: 0.8
      max_batch_size: 1
      max_model_len: 8192
      gpu_memory_utilization: 0.9
      async_max_concurrency: 8
      output_type: text

role_adapters:
  - adapter_id: dut_mmlu_pro_vllm_qwen
    role_type: dut_model
    backend_id: mmlu_pro_vllm_backend
    capabilities:
      - chat_completion

metrics:
  - metric_id: live_code_bench_pass
    implementation: live_code_bench_pass
    params:
      ks:
        - 1
        - 5
      cot_code_execution: false
      timeout: 6

tasks:
  - task_id: live_code_bench_eval
    dataset_id: live_code_bench_ds
    max_samples: 10
    reporting:
      sinks:
        - type: console
        - type: file
          params:
            output_path: ${GAGE_EVAL_SAVE_DIR:-./runs}/live_code_bench_vllm_events.jsonl
