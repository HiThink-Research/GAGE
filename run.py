#!/usr/bin/env python
"""Lightweight entrypoint for running gage-eval-main without the full CLI stack."""

from __future__ import annotations

import argparse
import os
import sys
import signal
import platform
import time
from pathlib import Path
from typing import Optional

import yaml

REPO_ROOT = Path(__file__).resolve().parent
SRC_ROOT = REPO_ROOT / "src"

if not SRC_ROOT.exists():
    raise RuntimeError(f"Expected gage-eval sources under {SRC_ROOT}, but the directory is missing.")

if str(SRC_ROOT) not in sys.path:
    sys.path.insert(0, str(SRC_ROOT))

import gage_eval  # noqa: F401 - auto-discovery happens on import
from gage_eval.config import build_default_registry
from gage_eval.config.pipeline_config import PipelineConfig
from gage_eval.evaluation.runtime_builder import build_runtime
from gage_eval.observability.trace import ObservabilityTrace
from gage_eval.role.resource_profile import NodeResource, ResourceProfile


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run a gage-eval PipelineConfig.")
    parser.add_argument(
        "--config",
        "-c",
        required=True,
        help="Path to the PipelineConfig YAML file.",
    )
    parser.add_argument(
        "--dataset-id",
        help="Optional dataset identifier override when multiple datasets are declared in the config.",
    )
    parser.add_argument(
        "--gpus",
        type=int,
        default=1,
        help="GPU count for the single-node ResourceProfile (default: 1).",
    )
    parser.add_argument(
        "--cpus",
        type=int,
        default=8,
        help="CPU count for the single-node ResourceProfile (default: 8).",
    )
    parser.add_argument(
        "--run-id",
        help="Optional run identifier; if omitted a random id will be generated by ObservabilityTrace.",
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        help="Override SampleLoop concurrency (otherwise auto-detected from GPU/CPU resources).",
    )
    parser.add_argument(
        "--model-path",
        help="Override VLLM native backend model path (sets VLLM_NATIVE_MODEL_PATH).",
    )
    parser.add_argument(
        "--output-dir",
        help="Optional directory for run artifacts; defaults to ./runs/<run_id>.",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        help="Optional max samples override; sets env GAGE_EVAL_MAX_SAMPLES.",
    )
    return parser.parse_args()


def _ensure_spawn_start_method() -> None:
    """Use 'spawn' to avoid CUDA re-init issues in forked processes."""

    try:
        import torch.multiprocessing as mp  # type: ignore
    except Exception:  # pragma: no cover - torch not installed or unavailable
        import multiprocessing as mp  # type: ignore

    current = mp.get_start_method(allow_none=True)
    if current == "spawn":
        return
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError as exc:  # pragma: no cover - defensive guard
        raise RuntimeError("gage-eval requires multiprocessing start method 'spawn' for CUDA workloads") from exc


def load_config(path: Path) -> dict:
    if not path.exists():
        raise FileNotFoundError(f"Config file '{path}' not found")
    with path.open("r", encoding="utf-8") as handle:
        data = yaml.safe_load(handle) or {}
    if not isinstance(data, dict):
        raise ValueError(f"Config '{path}' must be a mapping at the top level")
    return data


def ensure_save_dir(directory: Optional[str]) -> None:
    if not directory:
        return
    resolved = Path(directory).expanduser().resolve()
    resolved.mkdir(parents=True, exist_ok=True)
    os.environ["GAGE_EVAL_SAVE_DIR"] = str(resolved)


def _detect_gpu_count() -> Optional[int]:
    """Best-effort GPU count detection covering torch/nvidia-smi/env hints."""

    try:
        import torch

        if torch.cuda.is_available():
            count = torch.cuda.device_count()
            if count:
                return count
    except Exception:  # pragma: no cover - soft fallback
        pass

    env_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
    if env_visible:
        tokens = [token.strip() for token in env_visible.split(",") if token.strip() not in {"", "-1"}]
        if tokens:
            return len(tokens)

    env_world = os.environ.get("WORLD_SIZE")
    if env_world and env_world.isdigit():
        return int(env_world)

    for env_name in ("SLURM_GPUS_ON_NODE", "SLURM_JOB_GPUS"):
        env_value = os.environ.get(env_name)
        if env_value and env_value.isdigit():
            return int(env_value)

    try:
        import subprocess

        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=count", "--format=csv,noheader"],
            check=False,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            stripped = result.stdout.strip()
            if stripped.isdigit():
                return int(stripped)
    except FileNotFoundError:
        pass
    return None


def _ensure_default_concurrency(args: argparse.Namespace) -> None:
    if args.concurrency and args.concurrency > 0:
        os.environ["GAGE_EVAL_THREADS"] = str(args.concurrency)
        print(f"[gage-eval] concurrency set from CLI: {args.concurrency}")
        return
    if os.environ.get("GAGE_EVAL_THREADS"):
        return
    auto = _detect_gpu_count()
    if auto:
        os.environ["GAGE_EVAL_THREADS"] = str(max(1, auto))
        print(f"[gage-eval] auto-detected concurrency from GPU count: {auto}")
        return
    fallback = min(os.cpu_count() or 1, 4)
    os.environ["GAGE_EVAL_THREADS"] = str(fallback)
    print(f"[gage-eval] concurrency fallback to CPU-based default: {fallback}")


def _preflight_checks() -> None:
    """Best-effort环境健康度检查，避免脏显存/残留进程导致启动失败。"""

    try:
        import pynvml  # type: ignore

        pynvml.nvmlInit()
        device_count = pynvml.nvmlDeviceGetCount()
        for idx in range(device_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(idx)
            mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
            used_ratio = mem.used / mem.total if mem.total else 0.0
            if used_ratio > 0.1:
                procs = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
                if not procs:
                    print(
                        f"[gage-eval][CRITICAL] GPU{idx} mem used {mem.used/1024/1024:.0f}MiB "
                        f"({used_ratio:.0%}) but no compute processes detected. "
                        "Environment may be dirty; consider reboot/kill stale drivers."
                    )
        pynvml.nvmlShutdown()
    except Exception:
        # 如果没有 NVML，则忽略
        pass

    try:
        import psutil  # type: ignore

        me = psutil.Process()
        conflicts = []
        for proc in psutil.process_iter(["pid", "name", "cmdline"]):
            if proc.pid == me.pid:
                continue
            cmd = " ".join(proc.info.get("cmdline") or [])
            name = proc.info.get("name") or ""
            if "vllm" in cmd.lower() or "gage_eval" in cmd.lower() or "python -m gage_eval" in cmd.lower():
                conflicts.append((proc.pid, name or cmd))
        if conflicts:
            print("[gage-eval][WARNING] Potential conflicting processes detected:")
            for pid, cmd in conflicts:
                print(f"  - PID {pid}: {cmd}")
    except Exception:
        pass


_RUNTIME_REF: Optional[object] = None


def _install_signal_handlers():
    def _handler(signum, frame):
        global _RUNTIME_REF
        print(f"[gage-eval] Caught signal {signum}, shutting down…")
        try:
            if _RUNTIME_REF and hasattr(_RUNTIME_REF, "shutdown"):
                _RUNTIME_REF.shutdown()
        except Exception:
            pass
        try:
            import psutil  # type: ignore

            me = psutil.Process()
            for child in me.children(recursive=True):
                try:
                    child.kill()
                except Exception:
                    pass
        except Exception:
            pass
        os._exit(1)

    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
            signal.signal(sig, _handler)
        except Exception:
            pass


def _detect_hardware_profile() -> Optional[str]:
    """Best-effort硬件画像，返回 'h100' / 'apple' / None。"""

    try:
        import subprocess

        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
            check=False,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            names = [line.strip().lower() for line in result.stdout.splitlines() if line.strip()]
            if any("h100" in n for n in names):
                return "h100"
    except FileNotFoundError:
        pass
    sys_platform = platform.system().lower()
    machine = platform.machine().lower()
    if sys_platform == "darwin" and "arm" in machine:
        return "apple"
    return None


def _apply_hardware_profile_env(profile: Optional[str]) -> None:
    """基于硬件画像注入温和的并发/性能建议（仅在未显式指定时）。"""

    if profile == "h100":
        if not os.environ.get("GAGE_EVAL_THREADS"):
            os.environ["GAGE_EVAL_THREADS"] = "32"
            print("[gage-eval] hardware profile=h100 -> set GAGE_EVAL_THREADS=32 (override with --concurrency)")
    elif profile == "apple":
        if not os.environ.get("GAGE_EVAL_THREADS"):
            os.environ["GAGE_EVAL_THREADS"] = "4"
            print("[gage-eval] hardware profile=apple -> set GAGE_EVAL_THREADS=4 (override with --concurrency)")


def main() -> None:
    wall_clock_start = time.perf_counter()
    args = parse_args()
    _ensure_spawn_start_method()
    profile_hint = _detect_hardware_profile()
    _ensure_default_concurrency(args)
    _apply_hardware_profile_env(profile_hint)
    _preflight_checks()
    if args.max_samples is not None and args.max_samples > 0:
        os.environ["GAGE_EVAL_MAX_SAMPLES"] = str(args.max_samples)
    if args.model_path:
        os.environ["VLLM_NATIVE_MODEL_PATH"] = args.model_path
    _install_signal_handlers()
    config_payload = load_config(Path(args.config).expanduser())
    config = PipelineConfig.from_dict(config_payload)
    ensure_save_dir(args.output_dir)

    registry = build_default_registry()
    profile = ResourceProfile(nodes=[NodeResource(node_id="local", gpus=args.gpus, cpus=args.cpus)])
    trace = ObservabilityTrace(run_id=args.run_id)

    print(f"[gage-eval] building runtime from {args.config}")
    runtime = build_runtime(
        config=config,
        registry=registry,
        resource_profile=profile,
        dataset_id=args.dataset_id,
        trace=trace,
    )
    global _RUNTIME_REF
    _RUNTIME_REF = runtime
    if hasattr(runtime, "set_wall_clock_start"):
        try:
            runtime.set_wall_clock_start(wall_clock_start)  # type: ignore[call-arg]
        except Exception:
            pass
    print("[gage-eval] runtime ready, starting execution…")
    runtime.run()
    print("[gage-eval] run completed successfully.")


if __name__ == "__main__":
    main()
